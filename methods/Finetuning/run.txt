## all text train
accelerate launch src/train_bash.py \
    --stage sft \
    --do_train True \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --dataset simp_data\
    --template mistral \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir path_to_sft_checkpoint \
    --overwrite_cache  True \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --temperature 0.3 \
    --max_length 5000 \
    --learning_rate 5e-5 \
    --num_train_epochs 30.0 \
    --plot_loss True \
    --fp16 True


CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --do_train True \
    --do_eval False \
    --model_name_or_path /home/fdz/Desktop/chatglm3-6b/model/ZhipuAI/chatglm3-6b/ \
    --dataset car_qa \
    --template chatglm3 \
    --finetuning_type lora \
    --lora_target query_key_value \
    --output_dir path_to_sft_car_qa_60epo_checkpoint \
    --overwrite_cache \
    --overwrite_output_dir \
    --cutoff_len 1024 \
    --preprocessing_num_workers 16 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --warmup_steps 20 \
    --save_steps 100 \
    --learning_rate 5e-5 \
    --num_train_epochs 30.0 \
    --max_samples 3000 \
    --plot_loss \
    --fp16

accelerate launch src/train_bash.py \
    --stage sft \
    --do_train True \
    --model_name_or_path /home/fdz/Desktop/chatglm3-6b/model/ZhipuAI/chatglm3-6b/ \
    --dataset simp_data\
    --template chatglm3 \
    --finetuning_type lora \
    --lora_target query_key_value \
    --output_dir path_to_sft_car_qa_60epo_checkpoint \
    --overwrite_cache  True \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --temperature 0.6 \
    --max_length 5000 \
    --learning_rate 5e-5 \
    --num_train_epochs 60.0 \
    --plot_loss True \
    --fp16 True

python src/cli_demo.py \
    --model_name_or_path /home/fdz/Desktop/chatglm3-6b/model/ZhipuAI/chatglm3-6b/ \
    --template chatglm3 \

python src/cli_demo.py \
    --model_name_or_path /home/fdz/Desktop/chatglm3-6b/model/ZhipuAI/chatglm3-6b/ \
    --template chatglm3 \
    --adapter_name_or_path path_to_sft_car_qa_60epo_checkpoint \
    --finetuning_type lora


# 60 epoch temp 0.7
accelerate launch src/train_bash.py \
    --stage sft \
    --do_train True \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --dataset simp_data\
    --template mistral \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir path_to_sft_checkpoint_60epoch \
    --overwrite_cache  True \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --temperature 0.7 \
    --max_length 5000 \
    --learning_rate 5e-5 \
    --num_train_epochs 60.0 \
    --plot_loss True \
    --fp16 True


## discourse train
accelerate launch src/train_bash.py \
    --stage sft \
    --do_train True \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --dataset discourse_data\
    --template mistral \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir path_to_sft_checkpoint_discourse_60epoch \
    --overwrite_cache  True \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --temperature 0.7 \
    --max_length 5000 \
    --learning_rate 5e-5 \
    --num_train_epochs 60.0 \
    --plot_loss True \
    --fp16 True



python src/export_model.py \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --adapter_name_or_path path_to_sft_checkpoint \
    --template mistral \
    --finetuning_type lora \
    --export_dir path_to_export


python src/cli_demo.py \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --template mistral \


python src/cli_demo.py \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --adapter_name_or_path path_to_sft_checkpoint \
    --template mistral \
    --finetuning_type lora

python src/cli_demo.py \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --adapter_name_or_path path_to_sft_checkpoint_discourse_60epoch \
    --template mistral \
    --finetuning_type lora


## web test
python src/web_demo.py \
    --model_name_or_path /home/fdz/Desktop/mistral/Mistral/AI-ModelScope/Mistral-7B-Instruct-v0.1/ \
    --adapter_name_or_path path_to_sft_checkpoint \
    --template mistral \
    --finetuning_type lora



Please simplify the following English passage to make the meaning easier for 8th grade students to understand. You need to perform the following tasks:
1. Remove redundant paragraphs and add subheadings in appropriate places to make the overall structure of the article concise and organized.
2. Within each paragraph, remove redundant sentences, split excessively long sentences, or use other methods to modify sentence structures and make them concise and readable.
3. Replace complex expressions (words or phrases) used in the article with simpler and more commonly used expressions that convey the same meaning.
[NATIONAL HARBOR, Md. â€” Forty-six spellers in the 2014 Scripps National Spelling Bee will advance to Thursday's televised semifinals, including several who have been on the semifinal stage in prior years.\n\nAs their names were called, they lined up on the stage of the Gaylord National Resort and Convention Center with medals around their necks. Many appeared shocked they'd made it, as parents and supports shouted, cheered and cried with happiness.\n\nThe afternoon round began with 257 of the original 281 spellers. They successfully kept their chairs on stage with words like objicient (argumentative), Valkyrian (related to battle), formicivorous (feeding on ants), pikas (small mammals of the Asian mountains), ocellus (an insect eye used for detecting light) and Wordsworthian (relating to the poet William Wordsworth).\n\nA crowd favorite and the youngest speller, Hussain A. Godhrawala, 8, of Barnwell, South Carolina, seemed overwhelmed with joy when he got his word relating to nails or claws: unguiculate. Nine-year-old Kasey Cuenca Torres of San Angelo, Texas, got a rousing reaction to his winning word phaeton, an open automobile.\n\nWords that knocked out spellers in the second on-stage round included jeroboam (a wine bottle), obtundent (having the power to dull pain), palatalize (use of the tongue in producing sounds) and diphtheria (a childhood illness).\n\nWhile many relished their opportunities to banter with pronouncer Jacques Bailly, others seemed in a hurry to get away from the microphone. Instead of asking for languages of origin, parts of speech, or definitions, Nicole Seman, 13, of Leesburg, Virginia, Madeline Rickert, 13, of Minot, North Dakota, and Zander Reed, 11, of Ankeny, Iowa, went straight to correctly spelling their words: Faustian, connoisseur and Holstein, respectively.\n\nSemifinalists will take a further written test Wednesday evening.\n\nThe first stage round of the Scripps National Spelling Bee eliminated 24 of the 281 who began the day, with contest-ending stumbles on words such as Keeshond, a Dutch dog breed, and paradigm, referring to patterns.\n\nThe dramatic 87th running of the iconic American showdown over obscure words for exotic animals, unfamiliar foods, medical terms and other curiosities went 41 spellers before the first error. The young scholars, ages 8 to 15, successfully spelled such tongue-twisters as sassafras, cynosure, ipecac, balalaika and Panglossian.\n\nAs in prior years, the spellers often distinguished themselves with jaunty greetings for the longtime pronouncer, Bailly, such as 14-year-old Sriram Hathwar of Painted Post, New York, whose cheery, \"Hello, Dr. Bailly and friends,\" brought nervous laughter from the crowd in the grand ballroom of the Gaylord National Resort and Convention Center.\n\nAgain this year there are spellers who write invisible words on the backs of their name-and-number placards, on arms, on hands and in the air. Others bite lower lips, tousle their own hair or stick hands deep in pockets. Standouts sought to engage the crowd with their own brief stand-up comedy routine, as when Neha Seshadri, 13, of Detroit, asked Bailly: \"May I take a deep breath, please?\" When he said yes, she replied, \"Back to business.\"\n\nThe spellers hailed from all 50 states, the District of Columbia, Puerto Rico, U.S. overseas territories, Defense Department schools in Europe and seven foreign countries.\n\nThe annual event has over time become a major phenomenon with more than 1 million students participating in local bees leading to the regional winners who reach the national championship's limelight. The final rounds Thursday night will be televised live in prime time, and the winner can expect to appear on national morning news programs.\n\nSome spellers were clearly relieved at getting such common words as lieutenant, poignant, pyre, escargot, rapport, visceral, dichotomy and mistletoe, but also seemed to welcome such head-scratchers as galjoen (a sport fish of Africa), roodebok (a small antelope) and schottische (a polka-like dance).\n\nEd Horan, 14, of Hoboken, New Jersey, had the crowd breathless as he appeared stumped by the word for puppets moved by overhead strings, then nailed fantoccini. Similarly, Mary Polking, 14, of Charlotte, North Carolina, repeated the first few letters of the word for a man's felt hat, started over twice, then committed herself to the right vowel in the second syllable, and spelled homburg.\n\nSome of the words few use in conversation have become standards that the students have heard year after year befalling their predecessors, such as mandir (a Hindu temple), quisling (a turncoat), jeremiad (an angry tirade) foggara (an underground conduit for carrying water), tarragon (an aromatic spice), Baedeker (travel guidebooks), and bobbejaan (an Afrikaans word for baboon). The languages of origin for Wednesday's words ranged from Algonquin, Javanese and Yiddish to Russian, Dutch, Persian, Latin and Greek.\n\n---\n\nThe Scripps National Spelling Bee crowned two winners after a five-round duel in which neither could miss a word. In the end, Sriram Hathwar, of Painted Post, New York, and Ansun Sujoe, of Fort Worth, Texas, shared the prize.\n\nIt was just the fourth time in the Spelling Bee's history, and the first in more than 50 years, that winners shared the title. Others were in 1950, 1957 and 1962.\n\nBoth boys were magnanimous in victory, with Sriram saying the competition was against the dictionary not a human opponent. \"I'm happy to share this trophy with him,\" he said. Ansun said he had been happy just making the finals and \"even happier\" to have won.\n\nThe winning words were Sriram's stichomythia (dialogue in Greek drama) and Ansun's feuilleton (the features section of French newspapers).]
